async def forward_21(self, taskInfo):
    from collections import Counter
    import math
    import cmath

    sub_tasks = []
    agents = []
    logs = []

    # Stage 0: Geometry Setup and Symmetry Analysis

    # Subtask 0_1: Enumerate all chords (sides and diagonals) of the regular dodecagon
    debate_instr_0_1 = (
        "Sub-task 0_1: Precisely define and enumerate all line segments considered: list all sides and all diagonals "
        "of the regular dodecagon, explicitly clarifying that all diagonals connecting any two distinct vertices are included. "
        "Establish their geometric properties, including endpoints and chord lengths, without restricting to subsets or step increments. "
        "Given solutions to the problem from other agents, consider their opinions as additional advice. Please think carefully and provide an updated answer."
    )
    debate_agents_0_1 = [LLMAgentBase(["thinking", "answer"], "Debate Agent", model=self.node_model, role=role, temperature=0.0) for role in self.debate_role]
    all_thinking_0_1 = []
    all_answer_0_1 = []
    subtask_desc_0_1 = {
        "subtask_id": "subtask_0_1",
        "instruction": debate_instr_0_1,
        "context": ["user query"],
        "agent_collaboration": "Debate"
    }
    for r in range(self.max_round):
        for i, agent in enumerate(debate_agents_0_1):
            if r == 0:
                thinking, answer = await agent([taskInfo], debate_instr_0_1, r, is_sub_task=True)
            else:
                thinking, answer = await agent([taskInfo] + all_thinking_0_1[r-1], debate_instr_0_1, r, is_sub_task=True)
            agents.append(f"Debate agent {agent.id}, round {r}, thinking: {thinking.content}; answer: {answer.content}")
            if len(all_thinking_0_1) <= r:
                all_thinking_0_1.append([])
                all_answer_0_1.append([])
            all_thinking_0_1[r].append(thinking)
            all_answer_0_1[r].append(answer)
    final_decision_agent_0_1 = LLMAgentBase(["thinking", "answer"], "Final Decision Agent", model=self.node_model, temperature=0.0)
    final_instr_0_1 = "Sub-task 0_1: Given all the above thinking and answers, reason over them carefully and provide a final answer."
    thinking_0_1, answer_0_1 = await final_decision_agent_0_1([taskInfo] + all_thinking_0_1[-1], final_instr_0_1, is_sub_task=True)
    sub_tasks.append(f"Sub-task 0_1 output: thinking - {thinking_0_1.content}; answer - {answer_0_1.content}")
    subtask_desc_0_1['response'] = {"thinking": thinking_0_1, "answer": answer_0_1}
    logs.append(subtask_desc_0_1)
    print("Step 0_1: ", sub_tasks[-1])

    # Parse chords from answer_0_1 (assumed to be a structured list of chords with endpoints)
    # For demonstration, we generate chords programmatically here (replace with parsed data if available)
    n = 12
    vertices = [cmath.rect(1, 2*math.pi*i/n) for i in range(n)]
    chords = []
    for i in range(n):
        for j in range(i+1, n):
            chords.append((i, j, vertices[i], vertices[j]))  # (start_idx, end_idx, complex_point_start, complex_point_end)

    # Subtask 0_2: Enumerate all intersection points formed by sides and diagonals
    debate_instr_0_2 = (
        "Sub-task 0_2: Explicitly enumerate and characterize all intersection points formed by the sides and diagonals inside the dodecagon, "
        "including polygon vertices, all diagonal-diagonal intersections, and side-diagonal intersections. Record their precise coordinates and the chords they lie on, ensuring no intersection points are omitted regardless of step increments or symmetry assumptions. "
        "Given solutions to the problem from other agents, consider their opinions as additional advice. Please think carefully and provide an updated answer."
    )
    debate_agents_0_2 = [LLMAgentBase(["thinking", "answer"], "Debate Agent", model=self.node_model, role=role, temperature=0.0) for role in self.debate_role]
    all_thinking_0_2 = []
    all_answer_0_2 = []
    subtask_desc_0_2 = {
        "subtask_id": "subtask_0_2",
        "instruction": debate_instr_0_2,
        "context": ["user query", thinking_0_1.content],
        "agent_collaboration": "Debate"
    }
    for r in range(self.max_round):
        for i, agent in enumerate(debate_agents_0_2):
            if r == 0:
                thinking, answer = await agent([taskInfo, thinking_0_1], debate_instr_0_2, r, is_sub_task=True)
            else:
                thinking, answer = await agent([taskInfo, thinking_0_1] + all_thinking_0_2[r-1], debate_instr_0_2, r, is_sub_task=True)
            agents.append(f"Debate agent {agent.id}, round {r}, thinking: {thinking.content}; answer: {answer.content}")
            if len(all_thinking_0_2) <= r:
                all_thinking_0_2.append([])
                all_answer_0_2.append([])
            all_thinking_0_2[r].append(thinking)
            all_answer_0_2[r].append(answer)
    final_decision_agent_0_2 = LLMAgentBase(["thinking", "answer"], "Final Decision Agent", model=self.node_model, temperature=0.0)
    final_instr_0_2 = "Sub-task 0_2: Given all the above thinking and answers, reason over them carefully and provide a final answer."
    thinking_0_2, answer_0_2 = await final_decision_agent_0_2([taskInfo, thinking_0_1] + all_thinking_0_2[-1], final_instr_0_2, is_sub_task=True)
    sub_tasks.append(f"Sub-task 0_2 output: thinking - {thinking_0_2.content}; answer - {answer_0_2.content}")
    subtask_desc_0_2['response'] = {"thinking": thinking_0_2, "answer": answer_0_2}
    logs.append(subtask_desc_0_2)
    print("Step 0_2: ", sub_tasks[-1])

    # For demonstration, compute intersection points programmatically
    def line_intersection(p1, p2, p3, p4):
        # p1,p2 and p3,p4 are complex points defining two line segments
        # Returns intersection point if segments intersect, else None
        def det(a, b):
            return a.real * b.imag - a.imag * b.real
        d1 = p2 - p1
        d2 = p4 - p3
        denom = det(d1, d2)
        if abs(denom) < 1e-14:
            return None
        delta = p3 - p1
        t = det(delta, d2) / denom
        u = det(delta, d1) / denom
        if 0 <= t <= 1 and 0 <= u <= 1:
            return p1 + t * d1
        return None

    intersection_points = {}
    # Key: complex coordinate rounded, Value: dict with 'point', 'chords' (list of chord indices)
    def add_intersection(pt, chord_indices):
        key = (round(pt.real, 12), round(pt.imag, 12))
        if key not in intersection_points:
            intersection_points[key] = {"point": pt, "chords": set()}
        intersection_points[key]["chords"].update(chord_indices)

    # Add polygon vertices
    for idx, v in enumerate(vertices):
        add_intersection(v, [idx])

    # Add intersections of all chord pairs
    for i in range(len(chords)):
        for j in range(i+1, len(chords)):
            c1 = chords[i]
            c2 = chords[j]
            # Skip if chords share a vertex
            if c1[0] in (c2[0], c2[1]) or c1[1] in (c2[0], c2[1]):
                continue
            pt = line_intersection(c1[2], c1[3], c2[2], c2[3])
            if pt is not None:
                add_intersection(pt, [i, j])

    # Subtask 0_3: Formally establish geometric constraints for rectangles
    cot_sc_instruction_0_3 = (
        "Sub-task 0_3: Formally establish the geometric constraints for rectangles formed by these segments: verify that rectangles must have four right angles and opposite sides equal in length, "
        "and that each rectangle side lies exactly on one side or diagonal segment of the dodecagon. Avoid assumptions about vertex locations; allow rectangle vertices to be any enumerated intersection points."
    )
    cot_agents_0_3 = [LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0) for _ in range(self.max_sc)]
    possible_answers_0_3 = []
    possible_thinkings_0_3 = []
    subtask_desc_0_3 = {
        "subtask_id": "subtask_0_3",
        "instruction": cot_sc_instruction_0_3,
        "context": ["user query", thinking_0_1.content, thinking_0_2.content],
        "agent_collaboration": "SC_CoT"
    }
    for i in range(self.max_sc):
        thinking, answer = await cot_agents_0_3[i]([taskInfo, thinking_0_1, thinking_0_2], cot_sc_instruction_0_3, is_sub_task=True)
        agents.append(f"CoT-SC agent {cot_agents_0_3[i].id}, thinking: {thinking.content}; answer: {answer.content}")
        possible_answers_0_3.append(answer)
        possible_thinkings_0_3.append(thinking)
    final_decision_agent_0_3 = LLMAgentBase(["thinking", "answer"], "Final Decision Agent", model=self.node_model, temperature=0.0)
    final_instr_0_3 = "Sub-task 0_3: Given all the above thinking and answers, find the most consistent and correct solutions for the rectangle constraints."
    thinking_0_3, answer_0_3 = await final_decision_agent_0_3([taskInfo] + possible_thinkings_0_3, final_instr_0_3, is_sub_task=True)
    sub_tasks.append(f"Sub-task 0_3 output: thinking - {thinking_0_3.content}; answer - {answer_0_3.content}")
    subtask_desc_0_3['response'] = {"thinking": thinking_0_3, "answer": answer_0_3}
    logs.append(subtask_desc_0_3)
    print("Step 0_3: ", sub_tasks[-1])

    # Subtask 0_4: Analyze full symmetry group of the dodecagon
    cot_reflect_instruction_0_4 = (
        "Sub-task 0_4: Analyze the full symmetry group of the regular dodecagon, including all rotations and reflections, "
        "to understand how these symmetries act on the set of chords and intersection points. Prepare to apply these symmetries to identify equivalence classes of rectangles and reduce counting complexity without missing or double counting any unique rectangles. "
        "Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better."
    )
    cot_agent_0_4 = LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0)
    critic_agent_0_4 = LLMAgentBase(["feedback", "correct"], "Critic Agent", model=self.node_model, temperature=0.0)
    cot_inputs_0_4 = [taskInfo, thinking_0_1, thinking_0_3]
    subtask_desc_0_4 = {
        "subtask_id": "subtask_0_4",
        "instruction": cot_reflect_instruction_0_4,
        "context": ["user query", thinking_0_1.content, thinking_0_3.content],
        "agent_collaboration": "Reflexion"
    }
    thinking_0_4, answer_0_4 = await cot_agent_0_4(cot_inputs_0_4, cot_reflect_instruction_0_4, 0, is_sub_task=True)
    agents.append(f"Reflexion CoT agent {cot_agent_0_4.id}, thinking: {thinking_0_4.content}; answer: {answer_0_4.content}")
    for i in range(self.max_round):
        feedback, correct = await critic_agent_0_4([taskInfo, thinking_0_4], "Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output exactly 'True' in 'correct'", i, is_sub_task=True)
        agents.append(f"Critic agent {critic_agent_0_4.id}, feedback: {feedback.content}; correct: {correct.content}")
        if correct.content == "True":
            break
        cot_inputs_0_4.extend([thinking_0_4, feedback])
        thinking_0_4, answer_0_4 = await cot_agent_0_4(cot_inputs_0_4, cot_reflect_instruction_0_4, i + 1, is_sub_task=True)
        agents.append(f"Reflexion CoT agent {cot_agent_0_4.id}, refining, thinking: {thinking_0_4.content}; answer: {answer_0_4.content}")
    sub_tasks.append(f"Sub-task 0_4 output: thinking - {thinking_0_4.content}; answer - {answer_0_4.content}")
    subtask_desc_0_4['response'] = {"thinking": thinking_0_4, "answer": answer_0_4}
    logs.append(subtask_desc_0_4)
    print("Step 0_4: ", sub_tasks[-1])

    # Stage 1: Direction Vectors, Parallelism, Perpendicularity, and Rectangle Enumeration

    # Subtask 1_0: Compute and catalog all direction vectors of chords
    cot_sc_instruction_1_0 = (
        "Sub-task 1_0: Using the enumerated chords and intersection points, compute and catalog all direction vectors of chords (sides and diagonals). "
        "This data will support comprehensive parallelism and perpendicularity checks without relying on step-based assumptions."
    )
    cot_agents_1_0 = [LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0) for _ in range(self.max_sc)]
    possible_answers_1_0 = []
    possible_thinkings_1_0 = []
    subtask_desc_1_0 = {
        "subtask_id": "subtask_1_0",
        "instruction": cot_sc_instruction_1_0,
        "context": ["user query", thinking_0_1.content, thinking_0_2.content],
        "agent_collaboration": "SC_CoT"
    }
    for i in range(self.max_sc):
        thinking, answer = await cot_agents_1_0[i]([taskInfo, thinking_0_1, thinking_0_2], cot_sc_instruction_1_0, is_sub_task=True)
        agents.append(f"CoT-SC agent {cot_agents_1_0[i].id}, thinking: {thinking.content}; answer: {answer.content}")
        possible_answers_1_0.append(answer)
        possible_thinkings_1_0.append(thinking)
    final_decision_agent_1_0 = LLMAgentBase(["thinking", "answer"], "Final Decision Agent", model=self.node_model, temperature=0.0)
    final_instr_1_0 = "Sub-task 1_0: Given all the above thinking and answers, find the most consistent and correct solutions for direction vectors."
    thinking_1_0, answer_1_0 = await final_decision_agent_1_0([taskInfo] + possible_thinkings_1_0, final_instr_1_0, is_sub_task=True)
    sub_tasks.append(f"Sub-task 1_0 output: thinking - {thinking_1_0.content}; answer - {answer_1_0.content}")
    subtask_desc_1_0['response'] = {"thinking": thinking_1_0, "answer": answer_1_0}
    logs.append(subtask_desc_1_0)
    print("Step 1_0: ", sub_tasks[-1])

    # For demonstration, compute direction vectors programmatically
    chord_dirs = []
    for c in chords:
        vec = c[3] - c[2]
        length = abs(vec)
        unit_vec = vec / length
        chord_dirs.append(unit_vec)

    # Subtask 1_1: Identify all pairs of parallel chords
    debate_instr_1_1 = (
        "Sub-task 1_1: Identify all pairs of parallel chords (sides or diagonals) by comparing their direction vectors for exact parallelism, "
        "considering all enumerated chords. Avoid limiting to fixed step separations or subsets. Record all such parallel pairs as candidates for opposite sides of rectangles. "
        "Given solutions to the problem from other agents, consider their opinions as additional advice. Please think carefully and provide an updated answer."
    )
    debate_agents_1_1 = [LLMAgentBase(["thinking", "answer"], "Debate Agent", model=self.node_model, role=role, temperature=0.0) for role in self.debate_role]
    all_thinking_1_1 = []
    all_answer_1_1 = []
    subtask_desc_1_1 = {
        "subtask_id": "subtask_1_1",
        "instruction": debate_instr_1_1,
        "context": ["user query", thinking_1_0.content],
        "agent_collaboration": "Debate"
    }
    for r in range(self.max_round):
        for i, agent in enumerate(debate_agents_1_1):
            if r == 0:
                thinking, answer = await agent([taskInfo, thinking_1_0], debate_instr_1_1, r, is_sub_task=True)
            else:
                thinking, answer = await agent([taskInfo, thinking_1_0] + all_thinking_1_1[r-1], debate_instr_1_1, r, is_sub_task=True)
            agents.append(f"Debate agent {agent.id}, round {r}, thinking: {thinking.content}; answer: {answer.content}")
            if len(all_thinking_1_1) <= r:
                all_thinking_1_1.append([])
                all_answer_1_1.append([])
            all_thinking_1_1[r].append(thinking)
            all_answer_1_1[r].append(answer)
    final_decision_agent_1_1 = LLMAgentBase(["thinking", "answer"], "Final Decision Agent", model=self.node_model, temperature=0.0)
    final_instr_1_1 = "Sub-task 1_1: Given all the above thinking and answers, reason over them carefully and provide a final answer."
    thinking_1_1, answer_1_1 = await final_decision_agent_1_1([taskInfo, thinking_1_0] + all_thinking_1_1[-1], final_instr_1_1, is_sub_task=True)
    sub_tasks.append(f"Sub-task 1_1 output: thinking - {thinking_1_1.content}; answer - {answer_1_1.content}")
    subtask_desc_1_1['response'] = {"thinking": thinking_1_1, "answer": answer_1_1}
    logs.append(subtask_desc_1_1)
    print("Step 1_1: ", sub_tasks[-1])

    # Subtask 1_2: Find all pairs of chords perpendicular to the parallel pairs
    debate_instr_1_2 = (
        "Sub-task 1_2: For each pair of parallel chords identified, find all pairs of chords perpendicular to them by checking the dot product of direction vectors equals zero. "
        "This step must consider all chords and not restrict perpendicularity to fixed step differences. Record all perpendicular pairs that can serve as the other two sides of a rectangle. "
        "Given solutions to the problem from other agents, consider their opinions as additional advice. Please think carefully and provide an updated answer."
    )
    debate_agents_1_2 = [LLMAgentBase(["thinking", "answer"], "Debate Agent", model=self.node_model, role=role, temperature=0.0) for role in self.debate_role]
    all_thinking_1_2 = []
    all_answer_1_2 = []
    subtask_desc_1_2 = {
        "subtask_id": "subtask_1_2",
        "instruction": debate_instr_1_2,
        "context": ["user query", thinking_1_1.content],
        "agent_collaboration": "Debate"
    }
    for r in range(self.max_round):
        for i, agent in enumerate(debate_agents_1_2):
            if r == 0:
                thinking, answer = await agent([taskInfo, thinking_1_1], debate_instr_1_2, r, is_sub_task=True)
            else:
                thinking, answer = await agent([taskInfo, thinking_1_1] + all_thinking_1_2[r-1], debate_instr_1_2, r, is_sub_task=True)
            agents.append(f"Debate agent {agent.id}, round {r}, thinking: {thinking.content}; answer: {answer.content}")
            if len(all_thinking_1_2) <= r:
                all_thinking_1_2.append([])
                all_answer_1_2.append([])
            all_thinking_1_2[r].append(thinking)
            all_answer_1_2[r].append(answer)
    final_decision_agent_1_2 = LLMAgentBase(["thinking", "answer"], "Final Decision Agent", model=self.node_model, temperature=0.0)
    final_instr_1_2 = "Sub-task 1_2: Given all the above thinking and answers, reason over them carefully and provide a final answer."
    thinking_1_2, answer_1_2 = await final_decision_agent_1_2([taskInfo, thinking_1_1] + all_thinking_1_2[-1], final_instr_1_2, is_sub_task=True)
    sub_tasks.append(f"Sub-task 1_2 output: thinking - {thinking_1_2.content}; answer - {answer_1_2.content}")
    subtask_desc_1_2['response'] = {"thinking": thinking_1_2, "answer": answer_1_2}
    logs.append(subtask_desc_1_2)
    print("Step 1_2: ", sub_tasks[-1])

    # Subtask 1_3: Enumerate all candidate rectangles and verify properties
    cot_sc_instruction_1_3 = (
        "Sub-task 1_3: Enumerate all candidate rectangles formed by combining pairs of parallel chords with pairs of perpendicular chords, "
        "using the detailed intersection points as potential rectangle vertices. For each candidate, rigorously verify rectangle properties: four right angles, equal opposite sides, and that all four vertices lie on the enumerated intersection points. "
        "Use computational or algorithmic methods to exhaustively generate and test candidates, avoiding assumptions or shortcuts."
    )
    cot_agents_1_3 = [LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0) for _ in range(self.max_sc)]
    possible_answers_1_3 = []
    possible_thinkings_1_3 = []
    subtask_desc_1_3 = {
        "subtask_id": "subtask_1_3",
        "instruction": cot_sc_instruction_1_3,
        "context": ["user query", thinking_1_2.content, thinking_0_2.content, thinking_0_3.content],
        "agent_collaboration": "SC_CoT"
    }
    for i in range(self.max_sc):
        thinking, answer = await cot_agents_1_3[i]([taskInfo, thinking_1_2, thinking_0_2, thinking_0_3], cot_sc_instruction_1_3, is_sub_task=True)
        agents.append(f"CoT-SC agent {cot_agents_1_3[i].id}, thinking: {thinking.content}; answer: {answer.content}")
        possible_answers_1_3.append(answer)
        possible_thinkings_1_3.append(thinking)
    final_decision_agent_1_3 = LLMAgentBase(["thinking", "answer"], "Final Decision Agent", model=self.node_model, temperature=0.0)
    final_instr_1_3 = "Sub-task 1_3: Given all the above thinking and answers, find the most consistent and correct enumeration of rectangles."
    thinking_1_3, answer_1_3 = await final_decision_agent_1_3([taskInfo] + possible_thinkings_1_3, final_instr_1_3, is_sub_task=True)
    sub_tasks.append(f"Sub-task 1_3 output: thinking - {thinking_1_3.content}; answer - {answer_1_3.content}")
    subtask_desc_1_3['response'] = {"thinking": thinking_1_3, "answer": answer_1_3}
    logs.append(subtask_desc_1_3)
    print("Step 1_3: ", sub_tasks[-1])

    # Subtask 1_4: Apply symmetry group actions to identify equivalence classes
    cot_reflect_instruction_1_4 = (
        "Sub-task 1_4: Apply the symmetry group actions (rotations and reflections) from subtask_0_4 to the enumerated rectangles to identify equivalence classes and remove duplicates. "
        "Use group theory tools such as Burnsideâ€™s lemma or orbit-stabilizer methods to count unique rectangles accurately. Document the symmetry orbits and representatives clearly. "
        "Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better."
    )
    cot_agent_1_4 = LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0)
    critic_agent_1_4 = LLMAgentBase(["feedback", "correct"], "Critic Agent", model=self.node_model, temperature=0.0)
    cot_inputs_1_4 = [taskInfo, thinking_1_3, thinking_0_4]
    subtask_desc_1_4 = {
        "subtask_id": "subtask_1_4",
        "instruction": cot_reflect_instruction_1_4,
        "context": ["user query", thinking_1_3.content, thinking_0_4.content],
        "agent_collaboration": "Reflexion"
    }
    thinking_1_4, answer_1_4 = await cot_agent_1_4(cot_inputs_1_4, cot_reflect_instruction_1_4, 0, is_sub_task=True)
    agents.append(f"Reflexion CoT agent {cot_agent_1_4.id}, thinking: {thinking_1_4.content}; answer: {answer_1_4.content}")
    for i in range(self.max_round):
        feedback, correct = await critic_agent_1_4([taskInfo, thinking_1_4], "Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output exactly 'True' in 'correct'", i, is_sub_task=True)
        agents.append(f"Critic agent {critic_agent_1_4.id}, feedback: {feedback.content}; correct: {correct.content}")
        if correct.content == "True":
            break
        cot_inputs_1_4.extend([thinking_1_4, feedback])
        thinking_1_4, answer_1_4 = await cot_agent_1_4(cot_inputs_1_4, cot_reflect_instruction_1_4, i + 1, is_sub_task=True)
        agents.append(f"Reflexion CoT agent {cot_agent_1_4.id}, refining, thinking: {thinking_1_4.content}; answer: {answer_1_4.content}")
    sub_tasks.append(f"Sub-task 1_4 output: thinking - {thinking_1_4.content}; answer - {answer_1_4.content}")
    subtask_desc_1_4['response'] = {"thinking": thinking_1_4, "answer": answer_1_4}
    logs.append(subtask_desc_1_4)
    print("Step 1_4: ", sub_tasks[-1])

    # Subtask 1_5: Iterative review and validation of enumerated rectangles and symmetry classification
    cot_reflect_instruction_1_5 = (
        "Sub-task 1_5: Perform an iterative review and validation of the enumerated rectangles and symmetry classification by engaging Reflexion or Critic agents. "
        "Challenge assumptions, verify completeness, and refine the enumeration to ensure no rectangles are missed or double counted. Incorporate computational cross-checks or mini brute-force tests on subsets to validate the approach. "
        "Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better."
    )
    cot_agent_1_5 = LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0)
    critic_agent_1_5 = LLMAgentBase(["feedback", "correct"], "Critic Agent", model=self.node_model, temperature=0.0)
    cot_inputs_1_5 = [taskInfo, thinking_1_4]
    subtask_desc_1_5 = {
        "subtask_id": "subtask_1_5",
        "instruction": cot_reflect_instruction_1_5,
        "context": ["user query", thinking_1_4.content],
        "agent_collaboration": "Reflexion"
    }
    thinking_1_5, answer_1_5 = await cot_agent_1_5(cot_inputs_1_5, cot_reflect_instruction_1_5, 0, is_sub_task=True)
    agents.append(f"Reflexion CoT agent {cot_agent_1_5.id}, thinking: {thinking_1_5.content}; answer: {answer_1_5.content}")
    for i in range(self.max_round):
        feedback, correct = await critic_agent_1_5([taskInfo, thinking_1_5], "Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output exactly 'True' in 'correct'", i, is_sub_task=True)
        agents.append(f"Critic agent {critic_agent_1_5.id}, feedback: {feedback.content}; correct: {correct.content}")
        if correct.content == "True":
            break
        cot_inputs_1_5.extend([thinking_1_5, feedback])
        thinking_1_5, answer_1_5 = await cot_agent_1_5(cot_inputs_1_5, cot_reflect_instruction_1_5, i + 1, is_sub_task=True)
        agents.append(f"Reflexion CoT agent {cot_agent_1_5.id}, refining, thinking: {thinking_1_5.content}; answer: {answer_1_5.content}")
    sub_tasks.append(f"Sub-task 1_5 output: thinking - {thinking_1_5.content}; answer - {answer_1_5.content}")
    subtask_desc_1_5['response'] = {"thinking": thinking_1_5, "answer": answer_1_5}
    logs.append(subtask_desc_1_5)
    print("Step 1_5: ", sub_tasks[-1])

    # Stage 2: Aggregation and Final Consistency Check

    # Subtask 2_1: Aggregate final count of distinct rectangles
    cot_instruction_2_1 = (
        "Sub-task 2_1: Aggregate the final count of all distinct rectangles identified after symmetry reduction and validation. "
        "Provide a detailed report of the counting process, including the number of rectangles before and after symmetry considerations, and document the verification steps taken to ensure accuracy."
    )
    cot_agent_2_1 = LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0)
    subtask_desc_2_1 = {
        "subtask_id": "subtask_2_1",
        "instruction": cot_instruction_2_1,
        "context": ["user query", thinking_1_5.content],
        "agent_collaboration": "CoT"
    }
    thinking_2_1, answer_2_1 = await cot_agent_2_1([taskInfo, thinking_1_5], cot_instruction_2_1, is_sub_task=True)
    agents.append(f"CoT agent {cot_agent_2_1.id}, thinking: {thinking_2_1.content}; answer: {answer_2_1.content}")
    sub_tasks.append(f"Sub-task 2_1 output: thinking - {thinking_2_1.content}; answer - {answer_2_1.content}")
    subtask_desc_2_1['response'] = {"thinking": thinking_2_1, "answer": answer_2_1}
    logs.append(subtask_desc_2_1)
    print("Step 2_1: ", sub_tasks[-1])

    # Subtask 2_2: Final consistency check
    cot_reflect_instruction_2_2 = (
        "Sub-task 2_2: Conduct a final consistency check by comparing the aggregated count against geometric and combinatorial expectations, "
        "possibly using computational geometry tools or alternative enumeration methods. Confirm that the final answer is robust, consistent, and free from logical or computational errors. "
        "Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better."
    )
    cot_agent_2_2 = LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0)
    critic_agent_2_2 = LLMAgentBase(["feedback", "correct"], "Critic Agent", model=self.node_model, temperature=0.0)
    cot_inputs_2_2 = [taskInfo, thinking_2_1]
    subtask_desc_2_2 = {
        "subtask_id": "subtask_2_2",
        "instruction": cot_reflect_instruction_2_2,
        "context": ["user query", thinking_2_1.content],
        "agent_collaboration": "Reflexion"
    }
    thinking_2_2, answer_2_2 = await cot_agent_2_2(cot_inputs_2_2, cot_reflect_instruction_2_2, 0, is_sub_task=True)
    agents.append(f"Reflexion CoT agent {cot_agent_2_2.id}, thinking: {thinking_2_2.content}; answer: {answer_2_2.content}")
    for i in range(self.max_round):
        feedback, correct = await critic_agent_2_2([taskInfo, thinking_2_2], "Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output exactly 'True' in 'correct'", i, is_sub_task=True)
        agents.append(f"Critic agent {critic_agent_2_2.id}, feedback: {feedback.content}; correct: {correct.content}")
        if correct.content == "True":
            break
        cot_inputs_2_2.extend([thinking_2_2, feedback])
        thinking_2_2, answer_2_2 = await cot_agent_2_2(cot_inputs_2_2, cot_reflect_instruction_2_2, i + 1, is_sub_task=True)
        agents.append(f"Reflexion CoT agent {cot_agent_2_2.id}, refining, thinking: {thinking_2_2.content}; answer: {answer_2_2.content}")
    sub_tasks.append(f"Sub-task 2_2 output: thinking - {thinking_2_2.content}; answer - {answer_2_2.content}")
    subtask_desc_2_2['response'] = {"thinking": thinking_2_2, "answer": answer_2_2}
    logs.append(subtask_desc_2_2)
    print("Step 2_2: ", sub_tasks[-1])

    final_answer = await self.make_final_answer(thinking_2_2, answer_2_2, sub_tasks, agents)
    return final_answer, logs
