async def forward_22(self, taskInfo):
    from collections import Counter
    print("Task Requirement: ", taskInfo)
    sub_tasks = []
    agents = []
    logs = []

    cot_instruction_1 = (
        "Sub-task 1: Analyze the problem constraints: sum=30, unique mode=9, median is a positive integer not in the list. "
        "Explicitly consider all even list lengths n >= 4 that can accommodate at least two occurrences of 9. "
        "Justify why these lengths are valid and remove any unjustified exclusions. "
        "Prepare to consider n=4, 6, 8, and larger even lengths if relevant."
    )
    cot_agent_1 = LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0)
    subtask_desc1 = {
        "subtask_id": "subtask_1",
        "instruction": cot_instruction_1,
        "context": ["user query"],
        "agent_collaboration": "CoT"
    }
    thinking1, answer1 = await cot_agent_1([taskInfo], cot_instruction_1, is_sub_task=True)
    agents.append(f"CoT agent {cot_agent_1.id}, analyzing constraints, thinking: {thinking1.content}; answer: {answer1.content}")
    sub_tasks.append(f"Sub-task 1 output: thinking - {thinking1.content}; answer - {answer1.content}")
    subtask_desc1['response'] = {"thinking": thinking1, "answer": answer1}
    logs.append(subtask_desc1)
    print("Step 1: ", sub_tasks[-1])

    cot_sc_instruction_2 = (
        "Sub-task 2: Based on Sub-task 1, identify all possible median values that are positive integers not in the list. "
        "Determine how the median (average of two middle elements for even n) constrains the two middle elements, allowing any even difference between them as long as their average is an integer not in the list. "
        "Pass median gap information and candidate median values forward_22."
    )
    N_sc = self.max_sc
    cot_agents_2 = [LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.5) for _ in range(N_sc)]
    possible_answers_2 = []
    thinkingmapping_2 = {}
    answermapping_2 = {}
    subtask_desc2 = {
        "subtask_id": "subtask_2",
        "instruction": cot_sc_instruction_2,
        "context": ["user query", "thinking of subtask 1", "answer of subtask 1"],
        "agent_collaboration": "SC_CoT"
    }
    for i in range(N_sc):
        thinking2, answer2 = await cot_agents_2[i]([taskInfo, thinking1, answer1], cot_sc_instruction_2, is_sub_task=True)
        agents.append(f"CoT-SC agent {cot_agents_2[i].id}, identifying possible medians, thinking: {thinking2.content}; answer: {answer2.content}")
        possible_answers_2.append(answer2.content)
        thinkingmapping_2[answer2.content] = thinking2
        answermapping_2[answer2.content] = answer2
    answer2_content = Counter(possible_answers_2).most_common(1)[0][0]
    thinking2 = thinkingmapping_2[answer2_content]
    answer2 = answermapping_2[answer2_content]
    sub_tasks.append(f"Sub-task 2 output: thinking - {thinking2.content}; answer - {answer2.content}")
    subtask_desc2['response'] = {"thinking": thinking2, "answer": answer2}
    logs.append(subtask_desc2)
    print("Step 2: ", sub_tasks[-1])

    cot_sc_instruction_3 = (
        "Sub-task 3: Systematically and exhaustively enumerate all candidate lists of positive integers for each valid list length n identified in Sub-task 1, "
        "such that the sum of the list is 30, the median is as identified in Sub-task 2, and the list contains at least two 9s. "
        "Use programmatic or constraint programming methods to ensure no valid candidates are missed. "
        "Include frequency tables and median gap details in outputs for downstream validation."
    )
    cot_agents_3 = [LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.5) for _ in range(N_sc)]
    possible_answers_3 = []
    thinkingmapping_3 = {}
    answermapping_3 = {}
    subtask_desc3 = {
        "subtask_id": "subtask_3",
        "instruction": cot_sc_instruction_3,
        "context": ["user query", "thinking of subtask 2", "answer of subtask 2"],
        "agent_collaboration": "SC_CoT"
    }
    for i in range(N_sc):
        thinking3, answer3 = await cot_agents_3[i]([taskInfo, thinking2, answer2], cot_sc_instruction_3, is_sub_task=True)
        agents.append(f"CoT-SC agent {cot_agents_3[i].id}, enumerating candidate lists, thinking: {thinking3.content}; answer: {answer3.content}")
        possible_answers_3.append(answer3.content)
        thinkingmapping_3[answer3.content] = thinking3
        answermapping_3[answer3.content] = answer3
    answer3_content = Counter(possible_answers_3).most_common(1)[0][0]
    thinking3 = thinkingmapping_3[answer3_content]
    answer3 = answermapping_3[answer3_content]
    sub_tasks.append(f"Sub-task 3 output: thinking - {thinking3.content}; answer - {answer3.content}")
    subtask_desc3['response'] = {"thinking": thinking3, "answer": answer3}
    logs.append(subtask_desc3)
    print("Step 3: ", sub_tasks[-1])

    cot_instruction_4a = (
        "Sub-task 4a: Perform explicit frequency analysis on each candidate list from Sub-task 3 to verify that 9 is the unique mode, "
        "i.e., 9 appears strictly more times than any other number. Detect and exclude candidates with mode ties or multiple modes."
    )
    cot_agent_4a = LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0)
    subtask_desc4a = {
        "subtask_id": "subtask_4a",
        "instruction": cot_instruction_4a,
        "context": ["user query", "thinking of subtask 3", "answer of subtask 3"],
        "agent_collaboration": "CoT"
    }
    thinking4a, answer4a = await cot_agent_4a([taskInfo, thinking3, answer3], cot_instruction_4a, is_sub_task=True)
    agents.append(f"CoT agent {cot_agent_4a.id}, frequency analysis for unique mode, thinking: {thinking4a.content}; answer: {answer4a.content}")
    sub_tasks.append(f"Sub-task 4a output: thinking - {thinking4a.content}; answer - {answer4a.content}")
    subtask_desc4a['response'] = {"thinking": thinking4a, "answer": answer4a}
    logs.append(subtask_desc4a)
    print("Step 4a: ", sub_tasks[-1])

    cot_instruction_4b = (
        "Sub-task 4b: Validate the filtered candidate lists from Sub-task 4a against all remaining constraints: sum equals 30, median is a positive integer not in the list, and mode uniqueness is confirmed. "
        "Prepare a refined list of valid candidates."
    )
    cot_agent_4b = LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0)
    subtask_desc4b = {
        "subtask_id": "subtask_4b",
        "instruction": cot_instruction_4b,
        "context": ["user query", "thinking of subtask 4a", "answer of subtask 4a"],
        "agent_collaboration": "CoT"
    }
    thinking4b, answer4b = await cot_agent_4b([taskInfo, thinking4a, answer4a], cot_instruction_4b, is_sub_task=True)
    agents.append(f"CoT agent {cot_agent_4b.id}, validating candidates against all constraints, thinking: {thinking4b.content}; answer: {answer4b.content}")
    sub_tasks.append(f"Sub-task 4b output: thinking - {thinking4b.content}; answer - {answer4b.content}")
    subtask_desc4b['response'] = {"thinking": thinking4b, "answer": answer4b}
    logs.append(subtask_desc4b)
    print("Step 4b: ", sub_tasks[-1])

    debate_instruction_4c = (
        "Sub-task 4c: Conduct a debate/reflexion process involving multiple agents to cross-validate the candidate lists from Sub-task 4b, "
        "focusing on rigorous enforcement of mode uniqueness, sum constraints, and median conditions. Resolve any discrepancies and finalize the set of valid candidate lists."
    )
    debate_agents_4c = [LLMAgentBase(["thinking", "answer"], "Debate Agent", model=self.node_model, role=role, temperature=0.5) for role in self.debate_role]
    N_max_4c = self.max_round
    all_thinking4c = [[] for _ in range(N_max_4c)]
    all_answer4c = [[] for _ in range(N_max_4c)]
    subtask_desc4c = {
        "subtask_id": "subtask_4c",
        "instruction": debate_instruction_4c,
        "context": ["user query", "thinking of subtask 4b", "answer of subtask 4b"],
        "agent_collaboration": "Debate"
    }
    for r in range(N_max_4c):
        for i, agent in enumerate(debate_agents_4c):
            if r == 0:
                thinking4c, answer4c = await agent([taskInfo, thinking4b, answer4b], debate_instruction_4c, r, is_sub_task=True)
            else:
                input_infos_4c = [taskInfo, thinking4b, answer4b] + all_thinking4c[r-1] + all_answer4c[r-1]
                thinking4c, answer4c = await agent(input_infos_4c, debate_instruction_4c, r, is_sub_task=True)
            agents.append(f"Debate agent {agent.id}, round {r}, cross-validating candidates, thinking: {thinking4c.content}; answer: {answer4c.content}")
            all_thinking4c[r].append(thinking4c)
            all_answer4c[r].append(answer4c)
    final_decision_agent_4c = LLMAgentBase(["thinking", "answer"], "Final Decision Agent", model=self.node_model, temperature=0.0)
    thinking4c_final, answer4c_final = await final_decision_agent_4c([taskInfo] + all_thinking4c[-1] + all_answer4c[-1], "Sub-task 4c: Make final decision on valid candidate lists.", is_sub_task=True)
    agents.append(f"Final Decision agent, finalizing valid candidates, thinking: {thinking4c_final.content}; answer: {answer4c_final.content}")
    sub_tasks.append(f"Sub-task 4c output: thinking - {thinking4c_final.content}; answer - {answer4c_final.content}")
    subtask_desc4c['response'] = {"thinking": thinking4c_final, "answer": answer4c_final}
    logs.append(subtask_desc4c)
    print("Step 4c: ", sub_tasks[-1])

    cot_reflect_instruction_4d = (
        "Sub-task 4d: Perform a dedicated verification focusing on edge cases, especially the n=4 case, "
        "to ensure no valid solutions are missed. Re-examine candidates and constraints systematically for these cases."
    )
    cot_agent_4d = LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0)
    critic_agent_4d = LLMAgentBase(["feedback", "correct"], "Critic Agent", model=self.node_model, temperature=0.0)
    N_max_4d = self.max_round
    cot_inputs_4d = [taskInfo, thinking4c_final, answer4c_final]
    subtask_desc4d = {
        "subtask_id": "subtask_4d",
        "instruction": cot_reflect_instruction_4d,
        "context": ["user query", "thinking of subtask 4c", "answer of subtask 4c"],
        "agent_collaboration": "Reflexion"
    }
    thinking4d, answer4d = await cot_agent_4d(cot_inputs_4d, cot_reflect_instruction_4d, 0, is_sub_task=True)
    agents.append(f"Reflexion CoT agent {cot_agent_4d.id}, verifying edge cases, thinking: {thinking4d.content}; answer: {answer4d.content}")
    for i in range(N_max_4d):
        feedback, correct = await critic_agent_4d([taskInfo, thinking4d, answer4d],
                                                "please review the edge case verification and provide limitations.",
                                                i, is_sub_task=True)
        agents.append(f"Critic agent {critic_agent_4d.id}, feedback: {feedback.content}; correct: {correct.content}")
        if correct.content == "True":
            break
        cot_inputs_4d.extend([thinking4d, answer4d, feedback])
        thinking4d, answer4d = await cot_agent_4d(cot_inputs_4d, cot_reflect_instruction_4d, i + 1, is_sub_task=True)
        agents.append(f"Reflexion CoT agent {cot_agent_4d.id}, refining edge case verification, thinking: {thinking4d.content}; answer: {answer4d.content}")
    sub_tasks.append(f"Sub-task 4d output: thinking - {thinking4d.content}; answer - {answer4d.content}")
    subtask_desc4d['response'] = {"thinking": thinking4d, "answer": answer4d}
    logs.append(subtask_desc4d)
    print("Step 4d: ", sub_tasks[-1])

    cot_instruction_5 = (
        "Sub-task 5: For each valid candidate list finalized in Sub-task 4d, compute the sum of the squares of all items in the list."
    )
    cot_agent_5 = LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0)
    subtask_desc5 = {
        "subtask_id": "subtask_5",
        "instruction": cot_instruction_5,
        "context": ["user query", "thinking of subtask 4d", "answer of subtask 4d"],
        "agent_collaboration": "CoT"
    }
    thinking5, answer5 = await cot_agent_5([taskInfo, thinking4d, answer4d], cot_instruction_5, is_sub_task=True)
    agents.append(f"CoT agent {cot_agent_5.id}, calculating sum of squares, thinking: {thinking5.content}; answer: {answer5.content}")
    sub_tasks.append(f"Sub-task 5 output: thinking - {thinking5.content}; answer - {answer5.content}")
    subtask_desc5['response'] = {"thinking": thinking5, "answer": answer5}
    logs.append(subtask_desc5)
    print("Step 5: ", sub_tasks[-1])

    cot_reflect_instruction_6 = (
        "Sub-task 6: Verify the final answer(s) by rechecking all constraints against the chosen list(s) and confirm the correctness of the sum of squares calculation. "
        "Provide a final answer alongside a detailed verification report. If no valid list is found, explicitly state the reasons, highlighting failures in mode uniqueness or median conditions."
    )
    cot_agent_6 = LLMAgentBase(["thinking", "answer"], "Chain-of-Thought Agent", model=self.node_model, temperature=0.0)
    critic_agent_6 = LLMAgentBase(["feedback", "correct"], "Critic Agent", model=self.node_model, temperature=0.0)
    N_max_6 = self.max_round
    cot_inputs_6 = [taskInfo, thinking4d, answer4d, thinking5, answer5]
    subtask_desc6 = {
        "subtask_id": "subtask_6",
        "instruction": cot_reflect_instruction_6,
        "context": ["user query", "thinking of subtask 4d", "answer of subtask 4d", "thinking of subtask 5", "answer of subtask 5"],
        "agent_collaboration": "Reflexion"
    }
    thinking6, answer6 = await cot_agent_6(cot_inputs_6, cot_reflect_instruction_6, 0, is_sub_task=True)
    agents.append(f"Reflexion CoT agent {cot_agent_6.id}, verifying final answer, thinking: {thinking6.content}; answer: {answer6.content}")
    for i in range(N_max_6):
        feedback, correct = await critic_agent_6([taskInfo, thinking6, answer6],
                                                "please review the final answer verification and provide limitations.",
                                                i, is_sub_task=True)
        agents.append(f"Critic agent {critic_agent_6.id}, feedback: {feedback.content}; correct: {correct.content}")
        if correct.content == "True":
            break
        cot_inputs_6.extend([thinking6, answer6, feedback])
        thinking6, answer6 = await cot_agent_6(cot_inputs_6, cot_reflect_instruction_6, i + 1, is_sub_task=True)
        agents.append(f"Reflexion CoT agent {cot_agent_6.id}, refining verification, thinking: {thinking6.content}; answer: {answer6.content}")
    sub_tasks.append(f"Sub-task 6 output: thinking - {thinking6.content}; answer - {answer6.content}")
    subtask_desc6['response'] = {"thinking": thinking6, "answer": answer6}
    logs.append(subtask_desc6)
    print("Step 6: ", sub_tasks[-1])

    final_answer = await self.make_final_answer(thinking6, answer6, sub_tasks, agents)
    return final_answer, logs
