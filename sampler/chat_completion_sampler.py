import base64
import time
from typing import Any

import openai
from openai import OpenAI, AsyncOpenAI

from dataclasses import dataclass, field
from typing import Any

Message = dict[str, Any]  # keys role, content
MessageList = list[Message]


class SamplerBase:
    """
    Base class for defining a sampling model, which can be evaluated,
    or used as part of the grading process.
    """

    def __call__(self, message_list: MessageList) -> str:
        raise NotImplementedError


@dataclass
class EvalResult:
    """
    Result of running an evaluation (usually consisting of many samples)
    """

    score: float | None  # top-line metric
    metrics: dict[str, float] | None  # other metrics
    htmls: list[str]  # strings of valid HTML
    convos: list[MessageList]  # sampled conversations


@dataclass
class SingleEvalResult:
    """
    Result of evaluating a single sample
    """

    score: float | None
    metrics: dict[str, float] = field(default_factory=dict)
    html: str | None = None
    convo: MessageList | None = None  # sampled conversation


class Eval:
    """
    Base class for defining an evaluation.
    """

    def __call__(self, sampler: SamplerBase) -> EvalResult:
        raise NotImplementedError

OPENAI_SYSTEM_MESSAGE_API = "You are a helpful assistant."
OPENAI_SYSTEM_MESSAGE_CHATGPT = (
    "You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture."
    + "\nKnowledge cutoff: 2023-12\nCurrent date: 2024-04-01"
)


class ChatCompletionSampler(SamplerBase):
    """
    Sample from OpenAI's chat completion API
    """

    def __init__(
        self,
        model: str = "gpt-3.5-turbo",
        system_message: str | None = None,
        temperature: float = 0.5,
        # max_tokens: int = 1024,
    ):
        self.api_key_name = "OPENAI_API_KEY"
        self.client = AsyncOpenAI()
        # using api_key=os.environ.get("OPENAI_API_KEY")  # please set your API_KEY
        self.model = model
        self.system_message = system_message
        self.temperature = temperature
        # self.max_tokens = max_tokens
        self.image_format = "url"

    def _handle_image(
        self, image: str, encoding: str = "base64", format: str = "png", fovea: int = 768
    ):
        new_image = {
            "type": "image_url",
            "image_url": {
                "url": f"data:image/{format};{encoding},{image}",
            },
        }
        return new_image

    def _handle_text(self, text: str):
        return {"type": "text", "text": text}

    async def _pack_message(self, role: str, content: Any):
        return {"role": str(role), "content": content}

    async def __call__(self, message_list: MessageList, temperature=None, response_format=None) -> str:
        if self.system_message:
            message_list = [await self._pack_message("system", self.system_message)] + message_list
        trial = 0
        while True:
            try:
                for message_id, message in enumerate(message_list):
                    if type(message['content']) != str:
                        message_list[message_id]['content'] = str(message['content'])
                # print('message_list: ',message_list)

                if  response_format=='normal':
                    response = await self.client.chat.completions.create(
                        model=self.model,
                        messages=message_list,
                        temperature= temperature if temperature is not None else self.temperature,              
                        # max_tokens=self.max_tokens
                    )               
                else:
                    response = await self.client.chat.completions.create(
                        model=self.model,
                        messages=message_list,
                        temperature= temperature if temperature is not None else self.temperature,              
                        # max_tokens=self.max_tokens, 
                        response_format={"type": "json_object"}
                    )
                # print('response: ',response)
                return response.choices[0].message.content, response.usage
            # NOTE: BadRequestError is triggered once for MMMU, please uncomment if you are reruning MMMU
            except openai.BadRequestError as e:
                print("Bad Request Error", e)
                return ""
            except Exception as e:
                exception_backoff = 2**trial  # expontial back off
                print(
                    f"Rate limit exception so wait and retry {trial} after {exception_backoff} sec",
                    e,
                )
                time.sleep(exception_backoff)
                trial += 1
                if trial == 3: # basically mean it is bad request after 3 trials
                    print("Bad Request Error", e)
                    return ""
            # unknown error shall throw exception
            
    async def get_embeddings(self, text_list):
        """Get embeddings from OpenAI embedding model."""
        trial = 0
        while True:
            try:
                # Ensure all inputs are strings
                text_list = [str(text) for text in text_list]

                response = await self.client.embeddings.create(
                    model=self.model,
                    input=text_list
                )
                # Trả về embedding vector và usage (tokens used)
                embeddings = [item.embedding for item in response.data]
                return embeddings, response.usage

            except openai.BadRequestError as e:
                print("Bad Request Error:", e)
                return []
            except Exception as e:
                backoff = 2 ** trial
                print(f"Exception on embedding request. Retrying in {backoff} seconds...", e)
                time.sleep(backoff)
                trial += 1
                if trial == 3:
                    print("Giving up after 3 attempts. Error:", e)
                    return []
            # unknown error shall throw exception
